{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# Define the DQN (Deep Q-Network) model --> PLAY WITH IT\n",
    "class DQN(tf.keras.Model):\n",
    "    def __init__(self, action_size, state_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(4, activation='relu', input_shape=(state_size,))\n",
    "        self.fc2 = tf.keras.layers.Dense(16, activation='relu')\n",
    "        self.fc3 = tf.keras.layers.Dense(action_size, activation='linear')\n",
    "\n",
    "    def call(self, state):\n",
    "        x = self.fc1(state)\n",
    "        x = self.fc2(x)\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Setting agent and its learning parameters --> PLAY WITH IT\n",
    "class DQNAgent: \n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.learning_rate = 0.01\n",
    "        self.model = DQN(action_size, state_size)  # Use the DQN model class\n",
    "        self.model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        self.position = [0, 0]\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model(state, training=False)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            # Predict the target value for the current state\n",
    "            target = reward\n",
    "            if not done:\n",
    "                # Predict the target Q-value for the next state\n",
    "                target = (reward + self.gamma * np.amax(self.model.predict(next_state)[0]))\n",
    "            # Get the current prediction for all actions for current state\n",
    "            target_f = self.model.predict(state)\n",
    "            # Update the target for the action that was taken\n",
    "            target_f[0][action] = target\n",
    "            # Fit the model with the updated targets\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def update_position(self, new_position):\n",
    "            self.position = new_position\n",
    "\n",
    "def calculate_reward(new_position, nutrient_position, previous_distance):\n",
    "    # Calculate the Manhattan distance from the agent to the nutrient source\n",
    "    current_distance = abs(new_position[0] - nutrient_position[0]) + abs(new_position[1] - nutrient_position[1])\n",
    "    \n",
    "    # Reward function --> PLAY WITH IT\n",
    "    if current_distance < previous_distance:\n",
    "        reward_tmp = (100 / current_distance) if current_distance > 0 else 500\n",
    "        return reward_tmp # Moved closer\n",
    "    else:\n",
    "        reward_tmp = -(current_distance)\n",
    "        return reward_tmp # Moved away or stayed the same\n",
    "\n",
    "# Initialize DQN agent\n",
    "state_size = 2  # since the state is represented by x and y position\n",
    "action_size = 4  # ['up', 'down', 'left', 'right']\n",
    "dqn_agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "# Example of agent acting in the environment\n",
    "state = np.random.rand(1, state_size)  # dummy state\n",
    "action = dqn_agent.act(state)  # choose an action\n",
    "print(f\"Action taken by the DQN agent: {action}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Environment setup\n",
    "class GridWorld:\n",
    "    def __init__(self, size, nutrient_position, nutrient_gradient):\n",
    "        self.size = size\n",
    "        self.nutrient_position = nutrient_position\n",
    "        self.nutrient_gradient = nutrient_gradient\n",
    "\n",
    "        # Initialize the DQN agent within the GridWorld\n",
    "        state_size = 2  # For example, if using x and y coordinates as the state\n",
    "        action_size = 4  # Four possible actions: up, down, left, right\n",
    "        self.agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "        self.grid = np.zeros(size)\n",
    "        self._set_nutrient()\n",
    "\n",
    "    def _set_nutrient(self, gradient_type='diamond'):  # Choose 'radial', 'rectangular', or 'diamond'\n",
    "        # Set the nutrient source on the grid\n",
    "        x_center, y_center = self.nutrient_position\n",
    "        self.grid[x_center, y_center] = 1  # Mark the nutrient source\n",
    "\n",
    "        if gradient_type == 'rectangular':\n",
    "            for i in range(1, self.nutrient_gradient):\n",
    "                gradient_area = [(x_center+dx, y_center+dy) for dx in range(-i, i+1) for dy in range(-i, i+1) if 0 <= x_center+dx < self.size[0] and 0 <= y_center+dy < self.size[1]]\n",
    "                for pos in gradient_area:\n",
    "                    self.grid[pos] += 0.1  # Increase the value to represent the gradient\n",
    "\n",
    "        elif gradient_type == 'radial':\n",
    "            for dx in range(-self.nutrient_gradient, self.nutrient_gradient):\n",
    "                for dy in range(-self.nutrient_gradient, self.nutrient_gradient):\n",
    "                    dist = np.sqrt(dx**2 + dy**2)\n",
    "                    if dist < self.nutrient_gradient and 0 <= x_center+dx < self.size[0] and 0 <= y_center+dy < self.size[1]:\n",
    "                        self.grid[x_center+dx, y_center+dy] += (self.nutrient_gradient - dist) / self.nutrient_gradient\n",
    "\n",
    "        elif gradient_type == 'diamond':\n",
    "            for dx in range(-self.nutrient_gradient, self.nutrient_gradient):\n",
    "                for dy in range(-self.nutrient_gradient, self.nutrient_gradient):\n",
    "                    dist = abs(dx) + abs(dy)  # Manhattan distance for a diamond shape\n",
    "                    if dist < self.nutrient_gradient and 0 <= x_center+dx < self.size[0] and 0 <= y_center+dy < self.size[1]:\n",
    "                        self.grid[x_center+dx, y_center+dy] += (self.nutrient_gradient - dist) / self.nutrient_gradient\n",
    "\n",
    "    def show_grid(self, agent_position=None, step_number=None, ax=None):\n",
    "        if ax is None:\n",
    "            ax = plt.subplots(figsize=(8, 8))\n",
    "        plt.imshow(self.grid, cmap='hot', interpolation='nearest')\n",
    "\n",
    "        # If an agent's position is provided, overlay a green dot at the agent's position\n",
    "        if agent_position is not None:\n",
    "            # Ensure the agent's position is a flat list or tuple before plotting\n",
    "            agent_position = np.array(agent_position).flatten()\n",
    "            plt.scatter(agent_position[1], agent_position[0], color='green', s=100)  # Increase size for visibility\n",
    "\n",
    "        # If a step number is provided, display it on the plot\n",
    "        if step_number is not None:\n",
    "            plt.text(0.5, 0.95, f'Step: {step_number}', fontsize=14, color='black', ha='center', va='top', transform=plt.gcf().transFigure)\n",
    "\n",
    "        # Keep the axis but remove the colorbar\n",
    "        plt.xticks(ticks=np.arange(0, self.size[1], 10), labels=np.arange(0, self.size[1], 10))\n",
    "        plt.yticks(ticks=np.arange(0, self.size[0], 10), labels=np.arange(0, self.size[0], 10))\n",
    "        plt.xlabel('X-coordinate')\n",
    "        plt.ylabel('Y-coordinate')\n",
    "        plt.grid(False)  # Optionally turn off the grid if you prefer\n",
    "        plt.show()\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the agent to a new random position\n",
    "        new_position = (np.random.randint(0, self.size[0]), np.random.randint(0, self.size[1]))\n",
    "        self.agent.position = new_position\n",
    "        \n",
    "        # If there are other variables that should be reset at the start of each episode,\n",
    "        # they should be reset here as well.\n",
    "        \n",
    "        # Return the initial state of the agent\n",
    "        return self.agent.position\n",
    "    \n",
    "    def step(self, action_index):\n",
    "        # Map the action index to an actual movement\n",
    "        action_mapping = {\n",
    "            0: 'up',\n",
    "            1: 'down',\n",
    "            2: 'left',\n",
    "            3: 'right'\n",
    "        }\n",
    "        action = action_mapping[action_index]\n",
    "\n",
    "        # Get the current position of the agent\n",
    "        x, y = self.agent.position\n",
    "\n",
    "        # Update the agent's position based on the action\n",
    "        if action == 'up' and x > 0:\n",
    "            new_position = (x - 1, y)\n",
    "        elif action == 'down' and x < self.size[0] - 1:\n",
    "            new_position = (x + 1, y)\n",
    "        elif action == 'left' and y > 0:\n",
    "            new_position = (x, y - 1)\n",
    "        elif action == 'right' and y < self.size[1] - 1:\n",
    "            new_position = (x, y + 1)\n",
    "        else:\n",
    "            new_position = (x, y)  # No change if the action is not feasible\n",
    "\n",
    "        # Update the agent's position\n",
    "        self.agent.position = new_position\n",
    "\n",
    "        # Calculate the new distance to the nutrient source for the reward function\n",
    "        new_distance = abs(new_position[0] - self.nutrient_position[0]) + abs(new_position[1] - self.nutrient_position[1])\n",
    "        previous_distance = abs(x - self.nutrient_position[0]) + abs(y - self.nutrient_position[1])\n",
    "\n",
    "        # Calculate the reward\n",
    "        reward = calculate_reward(new_position, self.nutrient_position, previous_distance)\n",
    "        done = new_distance == 0\n",
    "\n",
    "        # Reshape the new position for the neural network input\n",
    "        new_state = np.array(new_position).reshape(1, -1)\n",
    "\n",
    "        return new_state, reward, done\n",
    "\n",
    "# Parameters\n",
    "grid_size = (100, 100)\n",
    "nutrient_source_position = (50, 50)  # Center of the grid\n",
    "nutrient_gradient = 100\n",
    "\n",
    "# Initialize the environment\n",
    "environment = GridWorld(grid_size, nutrient_source_position, nutrient_gradient)\n",
    "environment._set_nutrient(gradient_type='radial') \n",
    "\n",
    "# Show the initial grid world\n",
    "environment.show_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate and visualize the agent's movement for 100 steps without prior training\n",
    "\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "for step in range(100):\n",
    "    current_state = np.array(dqn_agent.position).reshape(1, -1)\n",
    "    \n",
    "    # Let the agent choose the action\n",
    "    action_index = dqn_agent.act(current_state)\n",
    "    \n",
    "    # Apply the action to the environment to get the new state and reward\n",
    "    new_state, reward, done = environment.step(action_index)\n",
    "    \n",
    "    # Update the agent's position\n",
    "    dqn_agent.update_position(new_state.flatten())\n",
    "\n",
    "    # Visualize the agent's movement on the grid\n",
    "    clear_output(wait=True)\n",
    "    environment.show_grid(dqn_agent.position, step_number=step)\n",
    "    \n",
    "    if done:\n",
    "        print(\"Found the nutrient source!\")\n",
    "        break\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRAINING \n",
    "\n",
    "from IPython.display import Image, display\n",
    "import io\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Training Parameters\n",
    "num_episodes = 500\n",
    "max_steps_per_episode = 25  # More reasonable limit for each episode\n",
    "batch_size = 4  # Batch size for training\n",
    "\n",
    "# Initialize the environment\n",
    "environment = GridWorld(grid_size, nutrient_source_position, nutrient_gradient)\n",
    "\n",
    "# Initialize DQN agent\n",
    "dqn_agent = DQNAgent(state_size=2, action_size=4)\n",
    "\n",
    "# Keep track of the learning progress\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "# To store the rewards for plotting\n",
    "rewards_per_episode = []\n",
    "\n",
    "# Store the frames for GIF\n",
    "frames = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Reset the environment and get the initial state\n",
    "    initial_state = environment.reset()  # This should return a tuple like (x, y)\n",
    "    state = np.array(initial_state).reshape(1, -1)  # Convert the tuple to a NumPy array and reshape it\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        # Agent selects an action\n",
    "        action_index = dqn_agent.act(state)\n",
    "        \n",
    "        # Apply the action to the environment\n",
    "        next_state, reward, done = environment.step(action_index)\n",
    "        next_state = next_state.reshape(1, -1)\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Remember the experience\n",
    "        dqn_agent.remember(state, action_index, reward, next_state, done)\n",
    "        \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Replay and train the neural network\n",
    "        if len(dqn_agent.memory) > batch_size:\n",
    "            dqn_agent.replay(batch_size)\n",
    "        \n",
    "        # Update the total_reward\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        print(step)\n",
    "    \n",
    "    # Decay epsilon\n",
    "    if dqn_agent.epsilon > dqn_agent.epsilon_min:\n",
    "        dqn_agent.epsilon *= dqn_agent.epsilon_decay\n",
    "    \n",
    "    all_epochs.append(step)\n",
    "    all_penalties.append(total_reward < 0)  # Assuming negative rewards are penalties\n",
    "\n",
    "    # Append the total reward of the episode for plotting\n",
    "    rewards_per_episode.append(total_reward)\n",
    "\n",
    "    # Visualize the training progress\n",
    "    clear_output(wait=True)\n",
    "    fig = plt.figure(figsize=(12, 5))\n",
    "    plt.plot(rewards_per_episode)\n",
    "    plt.title('Training Progress')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot to a BytesIO object\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    plt.close(fig)  # close the figure to prevent it from displaying in the notebook\n",
    "    buf.seek(0)\n",
    "    image = Image.open(buf)\n",
    "    display(image)\n",
    "\n",
    "    # Add to frames for GIF\n",
    "    frames.append(image)\n",
    "\n",
    "    # Print the progress\n",
    "    print(f\"Episode: {episode + 1}/{num_episodes}, Steps: {step}, Total reward: {total_reward}, Epsilon: {dqn_agent.epsilon:.2f}\")\n",
    "\n",
    "\n",
    "# Save the frames as a GIF\n",
    "imageio.mimsave('training_progress.gif', frames, fps=5)\n",
    "\n",
    "# After the training is complete\n",
    "dqn_agent.model.save('DQN_Agent_Diamond_pretrained.tf', save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import imageio\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load the trained model\n",
    "dqn_agent.model = load_model('DQN_Agent_Diamond_pretrained.tf')\n",
    "\n",
    "# Reset the environment for the demonstration\n",
    "start_position = environment.reset()\n",
    "state = np.array(start_position).reshape(1, -1)\n",
    "\n",
    "frames = []  # List to keep track of the frames for the GIF\n",
    "rewards = []  # List to keep track of the rewards\n",
    "\n",
    "# Simulate and visualize the agent's movement for 100 steps\n",
    "for t in range(100):\n",
    "    # Agent selects the best action based on the trained model\n",
    "    action_index = np.argmax(dqn_agent.model.predict(state))\n",
    "\n",
    "    # Apply the action to the environment\n",
    "    new_state, reward, done = environment.step(action_index)\n",
    "\n",
    "    # Update rewards list and state\n",
    "    rewards.append(reward)\n",
    "    state = new_state\n",
    "\n",
    "    # Visualization\n",
    "    clear_output(wait=True)\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))  # Adjust the subplot size ratio\n",
    "\n",
    "    # Plot for the agent's movement on the grid\n",
    "    ax[0].imshow(environment.grid, cmap='hot', interpolation='nearest')\n",
    "    agent_position = np.array(environment.agent.position).flatten()\n",
    "    ax[0].scatter(agent_position[1], agent_position[0], color='green', s=100)  # Mark the agent's position\n",
    "    ax[0].set_title('Agent Movement')\n",
    "    ax[0].set_xlabel('X-coordinate')\n",
    "    ax[0].set_ylabel('Y-coordinate')\n",
    "\n",
    "    # Plot for the reward per step\n",
    "    ax[1].plot(rewards, label='Reward')\n",
    "    ax[1].set_title('Reward per Step')\n",
    "    ax[1].set_xlabel('Step')\n",
    "    ax[1].set_ylabel('Reward')\n",
    "    ax[1].legend()\n",
    "\n",
    "    display(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Capture the current plot as an image frame\n",
    "    fig.canvas.draw()\n",
    "    image = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
    "    image = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    frames.append(image)\n",
    "\n",
    "    if done:\n",
    "        print(f\"Episode finished after {t + 1} timesteps\")\n",
    "        break\n",
    "\n",
    "# Save frames as a GIF\n",
    "imageio.mimsave('agent_movement --better.gif', frames, fps=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
